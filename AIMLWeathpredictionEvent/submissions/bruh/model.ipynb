{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing complete.\n",
      "X_train shape: torch.Size([17223, 21, 2]), y_train shape: torch.Size([17223, 2])\n",
      "X_test shape: torch.Size([4306, 21, 2]), y_test shape: torch.Size([4306, 2])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df['Date'] = pd.to_datetime(df['Full date'], format=\"%Y-%m-%d\")\n",
    "df['day_of_year'] = df['Date'].dt.dayofyear\n",
    "df['sine_day'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
    "df['cosine_day'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
    "\n",
    "df = df.drop(columns=['Date', 'day_of_year'])\n",
    "#print(df)\n",
    "features = ['sine_day', 'cosine_day']  \n",
    "target = ['Temp Max', 'Temp Min']\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[features + target] = scaler.fit_transform(df[features + target])\n",
    "\n",
    "sequence_length = 21\n",
    "X, y = [], []\n",
    "for i in range(sequence_length, len(df)):\n",
    "    X.append(df[features].iloc[i-sequence_length:i].values)\n",
    "    y.append(df[target].iloc[i].values)\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "print(\"Data preprocessing complete.\")\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=2, hidden_size=64, batch_first=True)  \n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=32, batch_first=True)  \n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(32, 32)  \n",
    "        self.fc2 = nn.Linear(32, 2)    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)  \n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x, _ = self.lstm2(x)  \n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = x[:, -1, :]  \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.0133, R²: 0.5488\n",
      "Epoch [2/50], Loss: 0.0066, R²: 0.7766\n",
      "Epoch [3/50], Loss: 0.0062, R²: 0.7891\n",
      "Epoch [4/50], Loss: 0.0061, R²: 0.7919\n",
      "Epoch [5/50], Loss: 0.0060, R²: 0.7949\n",
      "Epoch [6/50], Loss: 0.0059, R²: 0.7984\n",
      "Epoch [7/50], Loss: 0.0059, R²: 0.7984\n",
      "Epoch [8/50], Loss: 0.0058, R²: 0.8010\n",
      "Epoch [9/50], Loss: 0.0058, R²: 0.8013\n",
      "Epoch [10/50], Loss: 0.0058, R²: 0.8032\n",
      "Epoch [11/50], Loss: 0.0058, R²: 0.8035\n",
      "Epoch [12/50], Loss: 0.0057, R²: 0.8040\n",
      "Epoch [13/50], Loss: 0.0057, R²: 0.8052\n",
      "Epoch [14/50], Loss: 0.0057, R²: 0.8055\n",
      "Epoch [15/50], Loss: 0.0057, R²: 0.8053\n",
      "Epoch [16/50], Loss: 0.0057, R²: 0.8059\n",
      "Epoch [17/50], Loss: 0.0057, R²: 0.8057\n",
      "Epoch [18/50], Loss: 0.0057, R²: 0.8060\n",
      "Epoch [19/50], Loss: 0.0056, R²: 0.8078\n",
      "Epoch [20/50], Loss: 0.0056, R²: 0.8086\n",
      "Epoch [21/50], Loss: 0.0057, R²: 0.8075\n",
      "Epoch [22/50], Loss: 0.0056, R²: 0.8077\n",
      "Epoch [23/50], Loss: 0.0056, R²: 0.8082\n",
      "Epoch [24/50], Loss: 0.0056, R²: 0.8082\n",
      "Epoch [25/50], Loss: 0.0056, R²: 0.8087\n",
      "Epoch [26/50], Loss: 0.0056, R²: 0.8098\n",
      "Epoch [27/50], Loss: 0.0056, R²: 0.8090\n",
      "Epoch [28/50], Loss: 0.0056, R²: 0.8094\n",
      "Epoch [29/50], Loss: 0.0056, R²: 0.8091\n",
      "Epoch [30/50], Loss: 0.0056, R²: 0.8092\n",
      "Epoch [31/50], Loss: 0.0056, R²: 0.8099\n",
      "Epoch [32/50], Loss: 0.0056, R²: 0.8095\n",
      "Epoch [33/50], Loss: 0.0056, R²: 0.8100\n",
      "Epoch [34/50], Loss: 0.0056, R²: 0.8100\n",
      "Epoch [35/50], Loss: 0.0056, R²: 0.8103\n",
      "Epoch [36/50], Loss: 0.0056, R²: 0.8103\n",
      "Epoch [37/50], Loss: 0.0055, R²: 0.8113\n",
      "Epoch [38/50], Loss: 0.0056, R²: 0.8108\n",
      "Epoch [39/50], Loss: 0.0055, R²: 0.8110\n",
      "Epoch [40/50], Loss: 0.0055, R²: 0.8120\n",
      "Epoch [41/50], Loss: 0.0055, R²: 0.8113\n",
      "Epoch [42/50], Loss: 0.0055, R²: 0.8113\n",
      "Epoch [43/50], Loss: 0.0056, R²: 0.8108\n",
      "Epoch [44/50], Loss: 0.0055, R²: 0.8110\n",
      "Epoch [45/50], Loss: 0.0055, R²: 0.8111\n",
      "Epoch [46/50], Loss: 0.0055, R²: 0.8123\n",
      "Epoch [47/50], Loss: 0.0055, R²: 0.8110\n",
      "Epoch [48/50], Loss: 0.0055, R²: 0.8118\n",
      "Epoch [49/50], Loss: 0.0055, R²: 0.8120\n",
      "Epoch [50/50], Loss: 0.0055, R²: 0.8123\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def calculate_r2(y_true, y_pred):\n",
    "    return r2_score(y_true, y_pred)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = LSTMModel().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    \n",
    "    total_predictions = []\n",
    "    total_targets = []\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "        total_predictions.append(outputs.cpu().detach().numpy())\n",
    "        total_targets.append(targets.cpu().detach().numpy())\n",
    "\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "\n",
    "    total_predictions = np.concatenate(total_predictions)\n",
    "    total_targets = np.concatenate(total_targets)\n",
    "\n",
    "    r2 = calculate_r2(total_targets, total_predictions)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}, R²: {r2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.006498437132945077\n",
      "R² Score: 0.76789391040802\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_losses = []\n",
    "    predictions = []\n",
    "    targets_list = []  \n",
    "    \n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        test_losses.append(loss.item())\n",
    "        \n",
    "        \n",
    "        predictions.append(outputs.cpu().numpy())\n",
    "        targets_list.append(targets.cpu().numpy())\n",
    "\n",
    "\n",
    "test_loss = np.mean(test_losses)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "\n",
    "predictions = np.concatenate(predictions)\n",
    "targets = np.concatenate(targets_list)\n",
    "\n",
    "r2 = r2_score(targets, predictions)\n",
    "print(\"R² Score:\", r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in training data: Index(['id', 'Date', 'Rain', 'Temp Max', 'Temp Min', 'Full date', 'Year',\n",
      "       'Month'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "time data \"1\" doesn't match format \"%d-%m-%Y\", at position 0. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m train_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp Max\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m train_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp Min\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m train_df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp Max\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp Min\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are missing from the training data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday_of_year\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdayofyear\n\u001b[0;32m     14\u001b[0m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msine_day\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msin(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpi \u001b[38;5;241m*\u001b[39m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday_of_year\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m365\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\manug\\anaconda3\\envs\\py39_knime\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1063\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1061\u001b[0m             result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[1;32m-> 1063\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_listlike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n",
      "File \u001b[1;32mc:\\Users\\manug\\anaconda3\\envs\\py39_knime\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:247\u001b[0m, in \u001b[0;36m_maybe_cache\u001b[1;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[0;32m    245\u001b[0m unique_dates \u001b[38;5;241m=\u001b[39m unique(arg)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[1;32m--> 247\u001b[0m     cache_dates \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\manug\\anaconda3\\envs\\py39_knime\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:433\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64(\n\u001b[0;32m    436\u001b[0m     arg,\n\u001b[0;32m    437\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    441\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    442\u001b[0m )\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manug\\anaconda3\\envs\\py39_knime\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:467\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[1;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_array_strptime_with_fallback\u001b[39m(\n\u001b[0;32m    457\u001b[0m     arg,\n\u001b[0;32m    458\u001b[0m     name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    462\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    463\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[0;32m    464\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m     result, tz_out \u001b[38;5;241m=\u001b[39m \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tz_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    469\u001b[0m         unit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdatetime_data(result\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mstrptime.pyx:501\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstrptime.pyx:451\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstrptime.pyx:583\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime._parse_with_format\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: time data \"1\" doesn't match format \"%d-%m-%Y\", at position 0. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "print(\"Columns in training data:\", train_df.columns)  \n",
    "\n",
    "if 'Date' not in train_df.columns or 'Temp Max' not in train_df.columns or 'Temp Min' not in train_df.columns:\n",
    "    raise KeyError(\"Columns 'Date', 'Temp Max', or 'Temp Min' are missing from the training data\")\n",
    "\n",
    "train_df['Date'] = pd.to_datetime(train_df['Date'], format=\"%d-%m-%Y\")\n",
    "train_df['day_of_year'] = train_df['Date'].dt.dayofyear\n",
    "train_df['sine_day'] = np.sin(2 * np.pi * train_df['day_of_year'] / 365)\n",
    "train_df['cosine_day'] = np.cos(2 * np.pi * train_df['day_of_year'] / 365)\n",
    "\n",
    "features = ['sine_day', 'cosine_day']\n",
    "target = ['Temp Max', 'Temp Min']\n",
    "\n",
    "feature_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "train_df[features] = feature_scaler.fit_transform(train_df[features])\n",
    "\n",
    "train_df[target] = target_scaler.fit_transform(train_df[target])\n",
    "\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_df['Date'] = pd.to_datetime(test_df['Date'], format=\"%d-%m-%Y\")\n",
    "\n",
    "test_df['day_of_year'] = test_df['Date'].dt.dayofyear\n",
    "test_df['sine_day'] = np.sin(2 * np.pi * test_df['day_of_year'] / 365)\n",
    "test_df['cosine_day'] = np.cos(2 * np.pi * test_df['day_of_year'] / 365)\n",
    "\n",
    "test_df[features] = feature_scaler.transform(test_df[features])\n",
    "\n",
    "sequence_length = 21\n",
    "X_new = []\n",
    "dates_for_predictions = []\n",
    "\n",
    "for i in range(sequence_length, len(test_df)):\n",
    "    X_new.append(test_df[features].iloc[i-sequence_length:i].values)\n",
    "    dates_for_predictions.append(test_df['Date'].iloc[i])\n",
    "\n",
    "X_new = np.array(X_new)\n",
    "X_new = torch.tensor(X_new, dtype=torch.float32).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_new)\n",
    "\n",
    "predictions = predictions.cpu().numpy()\n",
    "predictions_original_scale = target_scaler.inverse_transform(predictions)\n",
    "\n",
    "output_df = pd.DataFrame(predictions_original_scale, columns=['Temp Max', 'Temp Min'])\n",
    "output_df['Date'] = dates_for_predictions\n",
    "output_df = output_df.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "output_df['Date'] = output_df['Date'].dt.strftime('%d-%m-%Y')\n",
    "output_df.to_csv('predicted_temperatures.csv', index=False)\n",
    "print(\"Predictions saved to 'predicted_temperatures.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_knime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
